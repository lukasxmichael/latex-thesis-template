\chapter{Theory}
\label{ch:theory}

\section{Bond Market Fundamentals}

\subsection{Primary and Secondary Market}

Corporate bonds serve as a crucial funding mechanism for medium and large enterprises. These financial instruments allow companies to leverage bond sales for expansion initiatives and diversify their financing sources. The bond market operates as two separate segments: the primary market, where new bonds are initially created and sold to investors, and the secondary market, where previously issued bonds are traded among market participants. Unlike equity trading, which predominantly occurs on exchanges that bring together multiple buyers and sellers, corporate bond trading has traditionally relied on market makers. These market makers—typically banks or broker-dealers—simultaneously provide bid and ask prices for the same bonds. When bondholders initiate a sell transaction, market makers offer a "bid" price; conversely, when investors seek to acquire bonds, market makers present an "ask" price. The differential between these prices, known as the "bid-ask spread," represents the potential profit for market makers and compensates them for the risk associated with holding bonds in their inventory.

\subsection{New Issuance Premium}

The new issue premium (NIP) represents a well-documented phenomenon in credit markets that is supported by both research and market practice, as explained by \parencite{TraczykNewFactor}. This premium typically occurs in standard bearer bullet structures distributed through a syndicated format. In this process, issuing companies engage a syndicate of banks to collect orders from potential investors within an order book. These issuances generally involve substantial volumes to establish sufficiently liquid secondary markets. To ensure adequate liquidity, syndicate banks aim for oversubscription in the order book by offering attractive pricing—specifically, a premium over comparable bonds in the secondary market. A positive new issue premium indicates a favorable yield spread over similar bonds, resulting in a lower initial price for the newly issued bond, which subsequently yields greater returns as its yield converges with peer group bonds shortly after issuance. While this premium is typically positive, its magnitude reflects the dynamics of price discovery during issuance, particularly the level of investor demand, which may occasionally result in negative premiums. Strong demand for a bond may enable issuers to secure funding at lower costs by allocating bonds at higher prices, thereby reducing effective yields and coupon rates.

From this framework, it becomes evident why exploiting the new issuance premium is appealing to investors. Many fixed-income fund managers regularly participate in new issuances to generate additional, relatively predictable returns for their portfolios. According to \textcite[chap. 35]{Fabozzi2021TheEdition}, investment teams commonly employ a relative-value credit approach to determine whether a bond carries a NIP. This analytical method compares the new bond's spread (such as z-spread, OAS, yield, etc.) against a basket of similarly rated bonds from the same sector with comparable maturity and seniority. As \textcite{fabozzi2025handbook} further observe, Market-Based vs. Model-Based Relative value constitutes an important consideration in this analysis. Fund managers typically participate when new issues trade wider than comparable bonds; otherwise, they abstain. However, this approach requires considerable experience for accurate predictions. Human error—particularly the failure to account for relevant factors that might indicate the absence of a NIP—can occur and potentially undermine fund performance.

\section{Machine Learning Fundamentals}

To better understand this paper's strategy, it is necessary to briefly examine the nature of machine learning, its operational mechanisms, and highlight XGBoost, the model employed in this research.

\textcite{Johari2018MachineExamples} defines machine learning as "a concept that allows the machine to learn from examples and experience, and that too without being explicitly programmed". She explains that machine learning inverts traditional programming approaches—rather than explicitly writing code, data is provided to generic algorithms that construct logic based on the supplied information. Machine learning algorithms represent an evolution of conventional algorithms, enhancing program intelligence by enabling autonomous learning from data. The process typically divides into two primary phases: the Training Phase and the Testing Phase. During training, the algorithm processes labeled data samples to identify patterns and relationships. The testing phase subsequently evaluates the model's performance using previously unseen data. Unlike training data, testing data is only employed after the model has been fully trained and refined. This data assesses the model's ability to generalize to novel scenarios and provides an unbiased evaluation of its accuracy, precision, and overall reliability.

XGBoost represents a highly effective machine learning methodology that has garnered significant attention for its exceptional performance in addressing classification and regression problems, as noted by \textcite{Harrison2023EffectiveModels}. This technique leverages an ensemble of decision trees to develop robust predictive models through iterative error minimization. In this context, gradient boosting refers to the process of constructing an initial decision tree and progressively incorporating additional trees, each designed to reduce errors from preceding trees. The process continues until reaching either a specified number of trees or a plateau in model improvement. Additionally, XGBoost employs regularization techniques to mitigate overfitting, enhancing the model's capacity to generalize effectively to new data. A distinctive feature of XGBoost is its capability to handle missing data through both adaptive and default handling mechanisms. Adaptive handling dynamically determines the optimal branch (left or right) for instances with missing values, while default handling directs missing values to the right branch during inference when no missing values were present in training data.

The selection of XGBoost for this research was informed by several valuable capabilities highlighted by \textcite{Harrison2023EffectiveModels}. The algorithm automatically imputes missing values within datasets, a significant advantage when API requests fail to return requested data, which commonly occurs when constructing large datasets. Furthermore, it effectively processes structured tabular data while offering parallel processing capabilities that accelerate training on large datasets by utilizing multiple CPU cores, making it particularly efficient for complex modeling tasks.