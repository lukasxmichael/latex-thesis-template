\chapter{Data and Feature Research}
\label{ch:Data and Feature Research}

This chapter examines the empirical foundation and methodological framework that underlie the analysis of NIP in European investment grade corporate bond markets. The investigation progresses from data acquisition and preprocessing through feature selection and model development to performance validation. Initially, the chapter explores the extraction and refinement of bond data from Refinitiv, addressing challenges related to data quality and temporal bias. Subsequently, a structured approach is developed to identify and evaluate potential predictive characteristics based on economic rationale and statistical significance. The analysis then constructs a machine learning framework using XGBoost that transforms these features into actionable investment signals, with particular attention to optimizing the precision-recall trade-off for maximum investment utility. The chapter concludes with a comprehensive backtest methodology that validates the model's efficacy under realistic market conditions. Throughout this analytical progression, the research maintains a consistent focus on translating complex statistical patterns into practical investment insights for fixed-income portfolio management.

\section{Data}
\label{sec:Data}

This study leverages comprehensive European corporate bond data extracted from the Refinitiv content layer API, which serves as the primary source for the training dataset used in the machine learning algorithm. Refinitiv provides thorough coverage of fixed income securities with detailed historical information, making it suitable for analyzing the new issue premium phenomenon. The initial universe of bond data was obtained using specific filtering criteria to ensure relevance to the research objectives. The parameters applied in the extraction query are presented in Table \ref{tab:filter_criteria}.

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{cc}\toprule
         \textbf{Bond Characteristic}& \textbf{Selection Criteria}\\\midrule
         Issuer Type& Corporate\\
         Status& Active and Inactive\\
         Bond Grade& Investment Grade\\
         Principal Currency& EUR\\
         Issue Date& 1/1/2000 - 1/5/2025\\ \bottomrule
    \end{tabular}
    \caption{Bond Selection Criteria for European Investment-Grade Corporate Bonds}
    \label{tab:filter_criteria}
\end{table}

This systematic filtering process yielded an initial data set comprising 30,800 bonds with issuance dates spanning January 2000 to May 2025. The extracted data were subsequently enriched with bond-specific characteristics and relevant market factors backdated to the time of issuance, creating a multidimensional feature matrix for analysis. Data quality varied considerably among extracted bonds. To ensure analytical integrity, only bonds with sufficient data quality were retained for subsequent analysis, resulting in a refined dataset of 7,320 bonds (approximately 24\%) of the original extraction. The primary criterion for inclusion was the availability of the data required to calculate the dependent variable. However, other features were allowed to have missing values, as the XGBoost algorithm used in this study has robust capabilities to handle incomplete data, as noted in Chapter \ref{ch:theory}.

There is noticeable temporal bias in the data distribution, as illustrated in Figure \ref{fig:data_availability}, with more recent years showing substantially higher bond representation. This phenomenon can be mainly attributed to the data retention policies of financial information providers, including Refinitiv, which typically maintain more comprehensive records of outstanding bonds compared to matured issues \parencite{Faberov2021RetrieveBond}. Consequently, earlier years in the analysis period contain fewer observations, as many bonds from these periods have already matured, and their complete historical data may not be fully preserved in the database.

\begin{figure}[h]
    \begin{center}
        \input{images/data_availability.pgf}
    \end{center}
    \caption{Temporal Distribution of European Corporate Bond Issuances in Dataset (2000-2025)}
    \label{fig:data_availability}
\end{figure}

The dataset extracted from the API showed several outliers. To address this issue, the Interquartile Range (IQR) was used using instructions from \textcite{Patil2023OutlierMethod}. For each numerical feature, the interquartile range was calculated and values that fall outside the limits defined by $Q1 - 1.5 \times IQR$ and $Q3 + 1.5 \times IQR$ were identified as outliers. Rather than removing entire observations, which would reduce the already limited dataset, the outlier values were replaced with NaN markers, allowing the XGBoost algorithm to handle these specific data points appropriately during model training.

Categorical variables required additional preprocessing to be compatible with the XGBoost algorithm, which operates exclusively on numerical inputs \parencite{Harrison2023EffectiveModels}. Specifically, the categorical features 'Seniority' and 'Region' underwent one-hot encoding transformation, converting each categorical variable into multiple binary indicator variables.

A detailed examination of missing values across features revealed substantial variation in the completeness of the data. Consumption and confidence metrics show the highest proportion of missing values, mainly because 2025 Q1 data were not yet available at the time of data extraction. Other features with significant missing values reflect limitations in the historical data coverage.

Following the outlier correction, the dependent variable (excess return) has a mean value of -7.24 basis points (bp), indicating that the newly issued bonds in the sample underperform their respective benchmarks during the five-day post-issuance period. Its standard deviation of 88.70 bp reflects substantial variation in this performance differential, with extreme values ranging from -246.85 bp to 230.54 bp. The negative mean and substantial variance align with \textcite{Traczyk2024NewFactor} finding that NIP occurs in different patterns across credit rating categories in the European corporate bond market, where it is "substantial for bonds with lower investment grade but it diminishes with rising rating quality" (p. 3). High-quality bonds tend to deliver negative premiums, while lower-rated securities demonstrate more consistent premium capture potential. This rating-dependent NIP underscores the need for more sophisticated selection approaches in investment-grade markets, where premium identification cannot rely on broad rating-class allocation, but requires nuanced analysis of issuer-specific factors.

\section{Methodology}
\label{sec:methodology}

The comprehensive data foundation established above provides the empirical basis for developing a systematic approach to identifying and quantifying the NIP. The methodological framework for feature selection, variable construction, and model evaluation is introduced here, integrating theoretical foundations with empirical validation to ensure analytical robustness.

The study's feature research considers economic rationale and stand-alone predictive power with regard to future short-term performance. The dependent variable for future performance is defined as the forward new issue price return in excess of the appropriate corporate bond index (FTSE Euro Broad Investment-Grade Bond Total Return Index EUR End of Day) return, both measured over the same period of $[t,\ t + d]$ with $t$ representing the issuance date. The research focuses on a five-day period $(d=5)$, which offers good utility in the investment-grade market.

To reduce the predictive problem to a classification rather than a regression, the excess return is categorized to enable analysis and modeling using machine learning classifiers. The specific classification framework is detailed in Section \ref{sec:model_signal_construction}.

For each feature, the evidence in the research data must support its economic basis and demonstrate its contribution to the NIP. Features with contradictory data were excluded, while those aligned with the rationale were assessed using:

\begin{enumerate}
    \item \textbf{Categorical analysis}: For categorical variables and variables that can be categorized based on intervals, the potential strength is assessed by evaluating the expected return per category. A large difference between categories indicates predictive power.
    \item \textbf{Correlation analysis}: The Spearman matrix is used to investigate the correlations between the features and between the features and the dependent variable.
    \item \textbf{Feature importance analysis}: The permutation importance "measures each feature's importance in making predictions using a decision tree model \parencite[p. 140]{Harrison2023EffectiveModels}. A positive importance value without relevant correlation implies a predictive relationship that linear regression cannot capture.
\end{enumerate}

The feature research yielded 21 relevant features (excluding two dependent variables), which were combined into a training dataset to predict binary excess return outcomes. Model performance is measured using the Confusion Matrix and excess returns of simulated investment strategies using out-of-sample data over a back-test period since 2024.

\section{Model and Signal Construction}
\label{sec:model_signal_construction}

Building on the methodological foundation for feature selection and evaluation, model architecture and signal generation procedures are developed to transform bond characteristics into actionable investment recommendations. This step bridges theoretical analysis with practical implementation by establishing classification boundaries and probability calibration techniques.

\subsection{Dependent Variable Classification}

In the dataset, performers and non-performers are approximately equally distributed, while the mean is still negative due to the higher weight of non-performers. However, to enhance the model's discriminative capabilities, the dataset is divided into a balanced ternary classification. This methodological decision allows for more nuanced differentiation between performance levels during model training.

\begin{table}[h]
\centering
\small
\begin{tabular}{cccccc}\toprule
\textbf{Binary} & \textbf{Ternary} & \textbf{Index} & \textbf{Proportion} & \textbf{min(xr)} & \textbf{max(xr)} \\\midrule
Loser & Underperformer & 0 & 52.27\% & \ldots & 0 bp \\
Winner & Moderate outperformer & 1 & 23.81\% & 0 bp & 51.01 bp \\
Winner & Strong outperformer & 2 & 23.81\% & 51.01 bp & \ldots \\ \bottomrule
\end{tabular}
\caption{Performance Classification Framework for Bond Excess Returns}
\label{tab:performance_classification}
\end{table}

As illustrated in Table \ref{tab:performance_classification}, the binary classification distinguishes between losers and winners, while the ternary classification further differentiates winners into two subcategories based on performance thresholds. This refined categorization serves as the foundation for the dependent variables used in model training.

\subsection{Machine Learning Methodology}

The predictive modeling approach employs XGBoost, selected for its demonstrated ability to capture complex nonlinear relationships between input features and categorical outcome variables. Empirical validation confirmed the superior predictive accuracy of XGBoost compared to traditional linear methods such as logistic regression, while simultaneously mitigating overfit concerns \parencite{Yang2024ComparisonClassification}. This conclusion was substantiated through monthly retraining and forward-looking out-of-sample validation.

The predictive function $P$ maps from the feature space $F$ to a probability distribution space $P: F \rightarrow \mathbb{R}^3$, defined as the direct probabilistic output of the XGBoost model:

\begin{equation}
P(x) = P_{\text{xgb}}(x)
\end{equation}

Here, $P_{\text{xgb}}(x)$ represents the predicted probabilities across the three performance categories (underperformer, moderate outperformer, strong outperformer), leveraging XGBoost's built-in multiclass probability objective \parencite{Guillaume2023HowClassification}. The model is trained using ternary classification to develop a nuanced understanding of feature relationships, although practical implementation focuses on binary predictions for investment decisions.

\subsection{Signal Construction}

The model generates investment signals through a systematic transformation of predicted probabilities into actionable recommendations. Given predicted probabilities $P(x) = [P_0, P_1, P_2]$ that sum up to one ($P_0 + P_1 + P_2 = 1$), the ternary signal $T: F \rightarrow \{0,1,2\}$ is defined as:

\begin{equation}
T(x) = \min \{ j : P_j(x) = \max_i P_i(x) \}
\end{equation}

This formulation selects the category with the highest probability, defaulting to the lowest index in cases of equal probabilities - a conservative approach aligned with practical investment requirements. For investment implementation, this ternary signal is converted to a binary signal $B(x)$:

\begin{equation}
B(x) =
\begin{cases}
1 & \text{if } T(x) > 0 \\
0 & \text{if } T(x) = 0
\end{cases}
\end{equation}

Since historical data exhibits a relatively balanced distribution between performing and nonperforming issuances, the model's utility derives primarily from its ability to differentiate between performance levels. This capability directly serves investment objectives, where identifying strong performers provides the greatest utility.

Model optimization focuses on balancing precision and recall metrics, defined in Table \ref{tab:precision_recall_definitions}.

\begin{table}[h]
\centering
\small
\begin{tabular}{ll}\toprule
\textbf{Metric} & \textbf{Interpretation} \\\midrule
Precision & Percentage of predicted outperformers correctly classified \\
Recall & Percentage of actual outperformers correctly identified \\\bottomrule
\end{tabular}
\caption{Performance Evaluation Metrics for Predictive Model Assessment}
\label{tab:precision_recall_definitions}
\end{table}

To achieve optimal investment performance, the recall is de prioritized by introducing a threshold parameter $0 < w \leq 1$, which modifies the original probabilities $P(x)$ into adjusted probabilities $P_w(x)$:

\begin{equation}
P_w(x) = [1 - w(1 - P_0), wP_1, wP_2]
\end{equation}

Decreasing $w$ imposes a more stringent classification threshold to identify outperformers, thus increasing precision at the expense of recall. The corresponding binary decision threshold $\tau$ is related to $w$ through $\tau = \frac{1}{2w}$.

\section{Optimization Strategy}
\label{sec:optimization}

The signal construction framework provides the foundation for developing an optimization approach that maximizes investment utility. Instead of relying on conventional classifier metrics in isolation, a comprehensive strategy is outlined that directly addresses the practical requirements of fixed income portfolio management through systematic parameter calibration.

The optimization seeks an optimal balance between precision and recall, specifically, the combination that maximizes overall investment utility, instead of solely maximizing precision at a fixed minimum recall. Optimization follows a two-step procedure that first establishes a robust probabilistic model and then identifies the optimal threshold parameter for investment decisions.

The initial step involves hyperparameter tuning of the ternary XGBoost classifier to create a robust probabilistic model $M = (P, T, B)$. Hyperparameters represent configuration variables that govern the learning process itself, not derived from the data during training but set prior to training to control model complexity and behavior. These include regularization terms, tree depth constraints, and learning rates that collectively determine the model's capacity to generalize beyond training data \parencite[p.40]{Harrison2023EffectiveModels}. This optimization was performed using Hyperopt, a Bayesian optimization framework that efficiently navigates the multidimensional parameter space through the sequential evaluation of promising configurations. Unlike exhaustive search methods, Hyperopt employs probabilistic modeling to identify promising regions of parameter space, progressively refining parameter estimates based on previous performance assessments and avoiding wasteful exploration of suboptimal configurations \parencite[pp. 85-87]{Harrison2023EffectiveModels}. This approach ensures systematic identification of optimal hyperparameters with significantly fewer evaluations, resulting in enhanced predictive performance. The first step establishes a classifier with maximal predictive performance, which provides a solid foundation for the probability-based model $M_w = (P_w, T_w, B_w)$.

The second stage focuses on optimizing the model with an emphasis on investment-related metrics, rather than solely aiming to enhance traditional classifier metrics such as precision or recall. This investment-driven approach is justified by several considerations. From a portfolio management perspective, the ultimate goal is to maximize the actual outcomes of investments. Although higher precision improves potential returns, market participation, which is the proportion of new issues predicted as outperformers, remains relevant due to practical considerations including diversification, capital deployment, and stability of returns. As increasing precision typically reduces market participation due to the precision-recall trade-off, a balanced solution becomes necessary. Given the lower proportion of outperforming issuances in the dataset, the model naturally optimizes towards lower market participation (approximately 10\%), reflecting a deliberate investment strategy that prioritizes higher precision through fewer, high-conviction positions.

Two strategies were initially considered for calibrating the threshold parameter: a static threshold optimized over the historical dataset, and a dynamically adjusted threshold recalibrated monthly. The static approach proved inadequate, failing to adapt to evolving market conditions, despite its methodological simplicity. In contrast, the purely dynamic approach introduced performance instability by over-reacting to transient market fluctuations.

The optimal solution emerged as a hybrid approach that balances historical stability with adaptability. The final implementation employs a weighted calibration strategy where the value of $w$ derived from the extensive training period contributes 80\%, while the most recent month's calibration provides the remaining 20\%:

\begin{equation}
\label{eq: weighted_calibration}
w_{\text{final}} = 0.8 \cdot w_{\text{training}} + 0.2 \cdot w_{\text{last month}}
\end{equation}

This calibration methodology ensures that the model remains predominantly anchored in robust historical patterns while maintaining sufficient adaptability to recent market movements. Empirical validation confirmed that this balanced approach consistently outperformed both pure static and pure dynamic calibration strategies. The 80/20 weighting effectively mitigates excessive reactivity to market fluctuations while preventing the model from becoming outdated in evolving market environments.

Investment utility metrics were explicitly defined as active return and market participation. Active return ($R_A$) represents the average return of predicted winners minus the average benchmark return for the same period, while market participation ($MP$) denotes the proportion of issues predicted to outperform. Given their direct relationship to precision and recall, the binary F1 score was chosen for hyperparameter optimization in the first step. The F1 score effectively balances precision and recall, which is particularly suitable given the balanced class distribution observed in the dataset.

The active return metric $R_{A,w}(X_t)$ for new issues $X_t \subseteq F$ during period $t$ is formally expressed as:

\begin{equation}
R_{A,w}(X_t) = \frac{\sum_{x \in X_t} T_w(x)r(x)}{m} - \frac{\sum_{x \in X_t} r(x)}{n}
\end{equation}

This equation quantifies the portfolio's risk-adjusted performance by comparing model-directed allocations against a benchmark strategy. The first term calculates the weighted average return across all bonds selected by the model, where the return of each bond $r(x)$ is weighted by its ternary signal value $T_w(x)$. The second term represents the average return that would be achieved by investing equally in all new available issues, effectively serving as the benchmark return. By subtracting this benchmark return, the equation isolates the excess performance attributable to the model's selection capabilities.

In the context of practical portfolio management, this metric directly evaluates the value added by the model's bond selection process, providing a clear measure of its economic significance in bp. For example, an active return of 15 bp indicates that the model-directed portfolio outperformed the equal-weighted benchmark by 0.15\% over the investment horizon.

Here, $r(x)$ is the total 5-day return realized from the issuance $x$, $m = \sum_{x \in X_t} T_w(x)$ represents the sum of the allocation weights, and $n = |X_t|$ is the total number of new issuances. The second term reflects the average benchmark return obtained when equally weighting all new issues in the market. Allocation according to the ternary signal $T_w(x)$ explicitly results in overweight of predicted strong performers compared to moderate performers at a ratio of $2:1$. This allocation strategy is central to the backtest and optimization objectives.

To evaluate longer-term model utility, multi-period averages of active return and market participation are calculated across $T$ periods:

\begin{equation}
\begin{split}
R_A(w) &= \frac{1}{T}\sum_{t=1}^{T} R_{A,w}(X_t), \quad MP(w) = \frac{1}{T}\sum_{t=1}^{T} MP_w(X_t), \\
&\text{where} \quad MP_w(X_t) = \frac{\sum_{x \in X_t} B_w(x)}{n}
\end{split}
\tag{4.7}
\end{equation}

The optimization objective function $f(w)$ used for threshold calibration is defined as the product of the average market participation and the average active return:

\begin{equation}
f(w) = MP(w) \times R_A(w)
\end{equation}

This investment-driven objective provides a practical framework for feature selection and model evaluation. By comparing optimization curves for different feature sets, each point represents a specific threshold configuration, with the optimal threshold corresponding to the peak of the curve.

\begin{figure}[h]
    \begin{center}
        \input{images/optimization_curve.pgf}
    \end{center}
    \caption{Investment Utility Optimization Curve Showing Relationship Between Threshold Parameter and Combined Market Participation and Active Return}
    \label{fig:optimization_curve}
\end{figure}

Figure \ref{fig:optimization_curve} illustrates the relationship between the threshold parameter ($w$) and the investment utility defined as the product of market participation and active return. The x-axis represents the threshold parameter ranging from 0.60 to 1.00, while the y-axis displays the average utility in bp. The curve demonstrates how investment utility increases with higher threshold values, reaching its maximum at $w^* = 0.96$. This pattern reveals a critical insight: As the threshold parameter approaches its upper limit, both the selectivity of the bond choices and the resulting active returns improve simultaneously. At $w^* = 0.96$, the model achieves nearly 2 bp of utility, representing the ideal balance between market participation and active return. This configuration enables portfolio managers to maximize risk-adjusted performance while maintaining sufficient diversification across new issuances.

From an investment implementation perspective, the optimization framework directly translates into practical portfolio management decisions. The threshold parameter $w$ effectively determines the selectivity of the bond selection process: Lower values create more stringent inclusion criteria, resulting in smaller but higher-conviction portfolios. Portfolio managers can adjust this parameter according to their investment mandates, risk preferences, and market conditions. For example, in volatile market environments, a more conservative approach (lower $w$) could be preferred to prioritize quality over quantity, while in stable markets with ample liquidity, a more inclusive approach (higher $w$) could be adopted to improve diversification and capital deployment.

\section{Backtest Methodology}

\textcite{Chen2024BacktestingDownsides} defines backtesting as the "general method for seeing how well a strategy or model would have done after the fact. It assesses the viability of a trading strategy by discovering how it would play out using historical data. If backtesting works, traders and analysts may have confidence to employ it going forward" (para. 1).

The analytical framework developed through data processing, feature selection, model construction, and parameter optimization is confirmed in a comprehensive backtest methodology that evaluates the model's predictive efficacy and investment utility. This section outlines the validation approach to assess performance under realistic market conditions.

The backtest employs an expanding-window cross-validation approach, structured into two main stages. The first stage involves optimization and training based on historical data, followed by a second stage that evaluates model performance using out-of-sample data. Each month, the window of historical training data expands by including the most recent data, simulating real-world investment conditions.

Starting from January 2024, the initial optimization stage involves hyperparameter tuning of the XGBoost classifier. This optimization leverages Hyperopt to efficiently identify the best-performing model configuration, thereby forming a robust baseline model. Following hyperparameter tuning, the optimal threshold parameter $w^*$ is applied as established in Section \ref{sec:optimization}. In the subsequent cross-validation stage, the trained model generates monthly investment predictions on unseen data using the hybrid calibration approach detailed in Equation \ref{eq: weighted_calibration}.

Investment outcomes are simulated monthly by selecting predicted outperforming issuances over a 5-day investment horizon. Strong predicted outperformers receive a higher allocation weight (a ratio of $2:1$) relative to moderate outperformers. The monthly performance of the investment strategy is evaluated using metrics such as active return, market participation, and precision.

After each month's evaluation, the backtest moves forward by one month, retraining the classifier with the expanded historical dataset while holding the hyperparameters constant. Each iteration applies the weighted threshold calibration methodology outlined in Section \ref{sec:optimization}.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=\textwidth]{images/backtest_timeline.pdf}
    \end{center}
    \caption{Expanding-Window Cross-Validation Methodology for Model Performance Assessment}
    \label{fig:backtest_timeline}
\end{figure}

This chapter established a comprehensive methodological framework for analyzing new issue premiums in European investment grade corporate bonds, progressing from data preprocessing through feature selection to model optimization and validation. The XGBoost-based approach, enhanced by ternary classification and hybrid threshold calibration, effectively captures complex relationships between bond characteristics and short-term performance. The resulting framework provides portfolio managers with a quantitative tool for systematic evaluation of new issues, representing a significant advancement over traditional qualitative approaches and establishing the foundation for subsequent feature analysis and performance evaluation.